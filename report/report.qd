.docname {Report p.1}
.doctype {paged}
.doclang {English}
.theme {paperwhite} layout:{latex}

.docauthors
  - Mounir Samite
    - email: mounir.samite@studio.unibo.it

.pagemargin {bottomcenter}
    .currentpage / .totalpages 

.align {center} 
    #! .docname 
    .docauthor 2026

---


.tableofcontents maxdepth:{2} 

# Problem analisys
The problem presents a computational task that is inherently parallelizable due to the independent nature of processing individual pdf files. The core challenge involves recursively traversing a directory structure, identifying pdf files, extracting text content, and searching for a specific word while maintaining accurate counts and providing real-time progress updates through a GUI.

## Decomposition strategy
The problem can be decomposed along two primary dimensions: task decomposition and data decomposition.
Task decomposition involves breaking down the workflow into distinct
operations:
- directory traversal
- file identification
- pdf text extraction
- word matching
- result aggregation

While data decomposition focuses on partitioning the set of pdf files
into chunks that can be processed concurrently.
The initial sequential implementation revealed that pdf files constitute
independent units of work, as analyzing one file does not depend on the
results of analyzing another, making them ideal candidates for parallel processing.

## Coordination and synchronization challenges
Despite the high degree of independence, several coordination
points require careful synchronization.
The shared counters (total files analyzed, PDFs found,
PDFs containing the target word) represent critical sections
that must be protected against race conditions when multiple threads
update them concurrently.
Additionally, the GUI updates must be coordinated to ensure
consistent and accurate display of progress without overwhelming
the event dispatch thread.
The start, stop, suspend, resume controls introduce additional
complexity, requiring mechanisms to pause and resume worker threads
cooperatively while maintaining system state consistency.


# Solution strategy and architecture
## High-level architecture
The project adopts a modular architecture that combines the Model-View-Controller 
pattern with the Strategy pattern to accommodate six distinct concurrency approaches while
maintaining code reusability and separation of concerns.

.mermaid 
    flowchart TD
        subgraph MVC["MVC Pattern"]
            V[SearchView]
            C[SearchController]
            M[SearchModel]
        end
        
        subgraph Strategies["Strategy Pattern"]
            S[PdfWordSearcher Interface]
            T1[Thread]
            T2[Virtual Thread]
            T3[Task Based]
            T4[Async Event]
            T5[Reactive]
            T6[Actor]
        end
        
        E[ExtractionEvent]
        
        V --> C
        C --> M
        M --> V
        C --> S
        
        S --> T1
        S --> T2
        S --> T3
        S --> T4
        S --> T5
        S --> T6
        
        T1 --> E
        T2 --> E
        T3 --> E
        T4 --> E
        T5 --> E
        T6 --> E
        
        E --> M

### Core architecture components
The application's structure consists of three primary MVC
components that remain consistent across all concurrency implementations:
- **SearchModel**: Encapsulates the application state, including counters for analyzed files, PDF files found, and matches containing the target word. This model serves as the single source of truth and notifies observers of state changes
- **SearchView**: Provides the graphical user interface with input fields for directory path and search word, control buttons (start/stop/suspend/resume), and output boxes displaying real-time progress
- **SearchController**: Mediates between the view and model, handling user interactions and delegating work to the appropriate concurrency strategy implementation

The ModelObserver interface implements the Observer pattern, enabling the view to react to model state changes without tight coupling, ensuring that GUI updates remain synchronized with the underlying computational progress.

### Strategy pattern
The core of this architecture lies in the strategies package,
which encapsulates each concurrency approach as an interchangeable implementation
of the PdfWordSearcher interface.
This design decision allows the controller to remain agnostic to the underlying concurrency
mechanism while supporting six fundamentally different approaches:
- **thread**: Implements traditional multithreaded approach using custom monitors and thread pools
- **virtual threads**: Leverages Java virtual threads for lightweight concurrency
- **task based**: Utilizes Java Executors and Fork/Join framework with task decomposition
- **async event**: Employs Vertx event-loop architecture for asynchronous processing
- **reactive programming**: Applies RxJava reactive streams for data flow management
- **actors**: Uses Akka actor model for message-passing concurrency

Each strategy package contains specialized components tailored to its concurrency model while adhering to a common interface,
ensuring seamless swapping between implementations.

### Event system
The events package defines a unified event model (ExtractionEvent and ExtractionEventType)
that enables consistent communication between Model, Controller and View.
This abstraction allows different strategies to report progress using the same event types
regardless of their internal implementation details.
â€‹
### Supporting infrastructure
The project structure includes dedicated components for testing and validation:

- generator: Contains scripts and sample PDFs for creating test datasets with varying file counts and directory depths
- pdfs: Houses multiple test scenarios ranging from small sets (3-10 files) to large-scale tests (50,000+ files) with both flat and recursive directory structures
- jpf-workspace: Provides Java PathFinder integration for formal verification of concurrent properties in the thread based implementation

This organizational structure ensures that each concurrency strategy can be developed,
tested, and analyzed independently while sharing common infrastructure components,
facilitating comparative performance analysis across all six approaches.

## Thread based approach
The thread based implementation leverages traditional multithreading
principles with custom monitor synchronization to achieve optimal CPU
utilization while maintaining strict control over concurrency mechanisms.

.mermaid 
    flowchart TD
        Start([PDF Files List])
        Split{Partition N=CPU+1}
        W1[Worker 1 Chunk 1]
        W2[Worker 2 Chunk 2]
        W3[Worker N Chunk N]
        P1[Process PDFs Extract Match]
        P2[Process PDFs Extract Match]
        P3[Process PDFs Extract Match]
        U1[Update Model: increment pdfsWithWord]
        U2[Update Model: increment pdfsWithWord]
        U3[Update Model: increment pdfsWithWord]
        M((Monitor))
        Wait[Output Thread await]
        Signal[signal complete]
        Result([Final Count + Time])
        
        Start --> Split
        Split --> W1
        Split --> W2
        Split --> W3
        W1 --> P1
        W2 --> P2
        W3 --> P3
        P1 --> U1
        P2 --> U2
        P3 --> U3
        U1 --> M
        U2 --> M
        U3 --> M
        M --> Wait
        Wait --> Signal
        Signal --> Result

### Thread pool size strategy
The solution dynamically determines the optimal number of worker threads based
on available CPU resources using the formula Nthreads = Ncpu + 1,
where Ncpu is obtained via `Runtime.getRuntime().availableProcessors()`.
This approach follows the established heuristic that a system with N processors
achieves optimal utilization with N+1 threads, accounting for potential I/O blocking during
PDF file operations.
When the number of PDF files is smaller than available CPUs,
the thread count adapts accordingly to avoid unnecessary thread creation overhead.

### Work partitioning and distribution
The ThreadPoolSearch class divides the list of PDF files into equal-sized chunks using a step-based partitioning strategy: step = numFiles / Nthreads.
Each Worker thread receives a contiguous range defined by start and end indices, processing files
independently without inter-worker communication. The final worker handles any remaining files due to integer division, ensuring all PDFs are processed.

### Monitor-Based Coordination

The custom monitor class implements a synchronization mechanism using ReentrantLock
and Condition variables from java.util.concurrent.locks, adhering to the assignment
constraint of using only low-level concurrent utilities for monitor implementation.
The monitor maintains two critical pieces of state:
- count: Accumulates the total number of PDFs containing the target word across all workers.
- numFiles: Tracks remaining unprocessed files, decremented as workers complete their chunks.

Workers call `updateFoundFiles(analyzedFiles, filesFound)` upon completing their assigned chunk, which atomically updates both counters under mutex protection. When the last worker finishes (numFiles == 0),
the monitor signals the waiting Output thread via the workersFinished condition variable.

### Worker thread implementation
Each worker thread `extends Thread` and processes its assigned file range by iterating through
PDF files, extracting text using Apache PDFBox's PDFTextStripper, and performing string matching.
The worker immediately updates the shared SearchModel when a match is found via `model.incCountPdfFilesWithWord()`,
enabling real-time updates.
After processing all assigned files, the worker reports its results to the monitor,
decoupling individual progress updates from final aggregation.

### Output thread and results collection

The Output thread implements a separate waiting mechanism that blocks on the monitor's get()
method until all workers signal completion. This design separates result collection from
computation, allowing the main thread to remain responsive while workers execute in parallel.
The output thread also measures total execution time from job start to completion,
providing performance metrics.

.mermaid 
    classDiagram
        class PdfWordSearcher {
            <<interface>>
            +extractText(List~File~ pdfs, String word, SearchModel model)
        }
        
        class ThreadPoolSearch {
            +extractText(List~File~ files, String word, SearchModel model)
        }
        
        class Monitor {
            -int count
            -Lock mutex
            -Condition workersFinished
            -int numFiles
            -SearchModel model
            +Monitor(int numFiles, SearchModel model)
            +updateFoundFiles(int analyzedFiles, int filesFound)
            +get() int
        }
        
        class Worker {
            -Monitor cell
            -List~File~ files
            -int start
            -int end
            -String searchedWord
            -SearchModel model
            +Worker(Monitor cell, int start, int end, List~File~ files, String word, SearchModel model)
            +run()
            -containsWord(File pdf, String word) boolean
        }
        
        class Output {
            -Monitor cell
            -long startTime
            +Output(Monitor cell, long startTime)
            +run()
        }
        
        class SearchModel {
            +incCountPdfFilesWithWord()
        }
        
        PdfWordSearcher <|.. ThreadPoolSearch
        ThreadPoolSearch ..> Monitor : creates
        ThreadPoolSearch ..> Worker : creates
        ThreadPoolSearch ..> Output : creates
        Worker --> Monitor : updates
        Worker --> SearchModel : updates
        Output --> Monitor : waits on
        Monitor --> SearchModel : uses

<<< 
## Virtual Threads

The virtual thread implementation adopts a virtual thread per file strategy where each
PDF is processed by its own virtual thread.

### Implementation Structure
The solution creates a **monitor** to coordinate all virtual threads, initialized with the total number of files to process. A **virtual thread executor** manages the lifecycle of all worker threads automatically through Java's resource management. Additionally, a separate **output thread** is created but held in reserve until all worker threads begin their execution.

The processing logic iterates through the entire list of PDF files and submits each one as an independent task to the executor. For each file, a new virtual thread is spawned with a unique identifier for debugging purposes. Each virtual thread independently loads its assigned PDF, extracts the text content, searches for the target word, and reports whether a match was found to the shared monitor.
Once all tasks are submitted, the output thread begins execution and the main program waits for it to complete before returning

### Monitor synchronization

The monitor uses explicit lock and condition variable mechanisms rather than traditional synchronized blocks, which is essential to prevent virtual threads from being pinned to their carrier threads during blocking operations. When each thread completes processing its file, it calls a method that updates the match counter if the word was found, decrements the remaining work counter, and signals when all files have been processed. The output thread blocks on the monitor until it receives the completion signal, then retrieves and returns the final count.

During the computation, every time a new file contains the word, the model is updated.

This architecture exploits the lightweight nature of virtual threads to achieve massive parallelism, creating as many virtual threads as there are PDF files, without overwhelming system resources.

.mermaid
    classDiagram
    class PdfWordSearcher {
        <<interface>>
        +extractText(List~File~ pdfs, String word, SearchModel model)
    }
    
    class VirtualThreadSearcher {
        +extractText(List~File~ files, String word, SearchModel model)
        -containsWord(File pdf, String word) boolean
    }
    
    class Monitor {
        -int count
        -Lock mutex
        -Condition workersFinished
        -int numFiles
        -SearchModel model
        +Monitor(int numFiles, SearchModel model)
        +foundWord(boolean found)
        +get() int
    }
    
    class VirtualThreadExecutor {
        <<executor>>
        +submit(Runnable task)
        +close()
    }
    
    class WorkerThread {
        <<virtual thread>>
        +start(Runnable task)
    }
    
    class OutputThread {
        <<virtual thread>>
        +run()
    }
    
    class SearchModel {
        +setCountPdfFilesWithWord(int count)
    }
    
    PdfWordSearcher <|.. VirtualThreadSearcher
    VirtualThreadSearcher ..> Monitor : creates
    VirtualThreadSearcher ..> VirtualThreadExecutor : uses
    VirtualThreadSearcher ..> OutputThread : creates
    VirtualThreadExecutor ..> WorkerThread : spawns N threads
    WorkerThread --> Monitor : reports to
    WorkerThread --> SearchModel : updates
    OutputThread --> Monitor : waits on
    Monitor --> SearchModel : updates

<<<

## Task based approach

The task-based implementation uses Java's ForkJoin framework to decompose the problem hierarchically, mirroring the recursive structure of the directory tree itself.

### Hierachical task decomposition
The solution begins by constructing a complete representation of the directory structure before processing begins. The tree representation recursively captures subdirectories and PDF files at each level, creating a hierarchical data structure that mirrors the file system. This upfront construction enables the ForkJoin framework to understand the complete workload structure and optimize task distribution.

The tree builder traverses each directory, classifying entries as either subdirectories (which are recursively processed) or PDF files (which are collected as leaf nodes). This separation allows the framework to spawn different task types: directory scanning tasks for structural traversal and word search tasks for actual file processing.

### ForkJoin execution model

The coordinator creates a ForkJoin pool and submits the root in the directory task, which initiates the recursive decomposition. Each directory scanning task examines its assigned directory node and performs a two phase forking strategy.

In the first phase, the task creates and forks child directory tasks for each subdirectory, enabling parallel exploration of the directory tree. In the second phase, it creates and forks word search tasks for each PDF file in the current directory. All forked tasks are collected in a list for subsequent joining.

After forking all subtasks, the directory task enters a joining phase where it waits for each child task to complete and accumulates their results. This fork-join pattern creates a recursive decomposition where work is divided (forked) down the tree and results are aggregated (joined) back up.


### Leaf (word search) task process
Word search tasks represent the atomic units of computation in this approach. Each task receives a single PDF file reference, extracts its text content, searches for the target word, and returns either 1 for a match or 0 for no match. When a match is found, the task immediately updates the shared model to provide constant UI feedback

.mermaid
    flowchart TD
    Start[ForkJoinSearcher] --> Build[Build DirectoryTree<br/>from root directory]
    Build --> Pool[Create ForkJoinPool]
    Pool --> RootTask[DirectoryScanTask<br/>Root Directory]
    
    RootTask --> Fork1{Fork Phase}
    
    Fork1 --> SubDir1[DirectoryScanTask<br/>Subdirectory 1]
    Fork1 --> SubDir2[DirectoryScanTask<br/>Subdirectory 2]
    Fork1 --> PDF1[WordSearchTask<br/>PDF 1]
    Fork1 --> PDF2[WordSearchTask<br/>PDF 2]
    Fork1 --> PDF3[WordSearchTask<br/>PDF 3]
    
    SubDir1 --> Fork2{Fork Phase}
    Fork2 --> PDF4[WordSearchTask<br/>PDF 4]
    Fork2 --> PDF5[WordSearchTask<br/>PDF 5]
    
    SubDir2 --> Fork3{Fork Phase}
    Fork3 --> PDF6[WordSearchTask<br/>PDF 6]
    Fork3 --> PDF7[WordSearchTask<br/>PDF 7]
    
    PDF1 --> R1[Return 0/1]
    PDF2 --> R2[Return 0/1]
    PDF3 --> R3[Return 0/1]
    PDF4 --> R4[Return 0/1]
    PDF5 --> R5[Return 0/1]
    PDF6 --> R6[Return 0/1]
    PDF7 --> R7[Return 0/1]
    
    R4 --> Join2[Join Results]
    R5 --> Join2
    Join2 --> SubDir1Result[Sum]
    
    R6 --> Join3[Join Results]
    R7 --> Join3
    Join3 --> SubDir2Result[Sum]
    
    R1 --> Join1[Join All Results]
    R2 --> Join1
    R3 --> Join1
    SubDir1Result --> Join1
    SubDir2Result --> Join1
    
    Join1 --> Final[Total Count]
    
<<<
## Async event approach
The async event implementation uses the Vert.x framework, which provides an event loop architecture for non-blocking asynchronous processing.

### Verticle based architecture
The solution creates a custom verticle, which serves as the deployable unit of concurrent execution in Vert.x. The main searcher class configures a Vert.x instance with a worker pool sized to match available CPU cores, then deploys the PDF search verticle onto this runtime.
The verticle encapsulates the entire search logic within its methods, receiving the list of PDF files, target word, and model reference through its constructor. When the verticle starts, it initiates the asynchronous processing pipeline.

### Event loop and worker pool

For each PDF file, the verticle submits a blocking task that extracts text and searches for the target word. These tasks execute on worker threads from the pool, preventing the event loop from blocking. Each blocking operation immediately returns a Future object representing the eventual completion of that task.

### Future Composition and aggregation
All individual file processing futures are collected into a list as they are created. The framework then composes these futures using a composite future that completes only when all individual futures have finished.
When the composite future completes, a mapping operation iterates through all results, summing the match counts and updating the model with the final total. This composition is declarative, with success handlers attached to process results when they become available.

### Non blocking result handling
The result handling follows a callback based pattern where success handlers are registered to execute when futures complete. This approach ensures the event loop remains free to handle other events while waiting for blocking operations to finish. Timing information is captured at the start of processing and compared against completion time to measure total execution duration.

.mermaid 
    flowchart TD
    Start[VertxAsyncSearcher] --> Config[Configure Vert.x<br/>Worker Pool Size = CPU cores]
    Config --> Deploy[Deploy PdfSearchVerticle]
    
    Deploy --> EventLoop[Event Loop Thread]
    
    EventLoop --> Submit1[Submit Blocking Task<br/>PDF 1]
    EventLoop --> Submit2[Submit Blocking Task<br/>PDF 2]
    EventLoop --> Submit3[Submit Blocking Task<br/>PDF N]
    
    Submit1 --> Future1[Future 1]
    Submit2 --> Future2[Future 2]
    Submit3 --> FutureN[Future N]
    
    Future1 --> Worker1[Worker Thread 1<br/>Extract & Search PDF 1]
    Future2 --> Worker2[Worker Thread 2<br/>Extract & Search PDF 2]
    FutureN --> WorkerN[Worker Thread N<br/>Extract & Search PDF N]
    
    Worker1 --> Update1[Update Model<br/>if match found]
    Worker2 --> Update2[Update Model<br/>if match found]
    WorkerN --> UpdateN[Update Model<br/>if match found]
    
    Update1 --> Result1[Return 0/1]
    Update2 --> Result2[Return 0/1]
    UpdateN --> ResultN[Return 0/1]
    
    Result1 --> Complete1[Future 1 Completes]
    Result2 --> Complete2[Future 2 Completes]
    ResultN --> CompleteN[Future N Completes]
    
    Complete1 --> Composite[Future.all<br/>Composite Future]
    Complete2 --> Composite
    CompleteN --> Composite
    
    Composite --> AllComplete{All Futures<br/>Complete?}
    AllComplete -->|Yes| Map[Sum all results]
    Map --> Success[Success Handler<br/>Print total & time]
    
    AllComplete -->|No| Wait[Event Loop Waits]
    Wait -.-> Composite

<<<
## Reactive programming approach
The reactive programming implementation uses RxJava to model PDF processing as a data stream. 

### Hot observable stream creation
The solution creates a hot observable stream by first defining a flowable source that emits processing results. The flowable is constructed using a custom emitter that iterates through all PDF files sequentially, processing each one and emitting integer values: 1 for files containing the target word, 0 otherwise.
The emitter handles the complete lifecycle: it processes all files in a loop, emits results as they become available, signals completion when all files are processed, and propagates any errors encountered during execution. A **buffer backpressure strategy** is configured to handle situations where the consumer cannot keep up with emitted items.
After creating the flowable, it is converted to a hot observable using the publish operator and immediately connected. 

### flow control 
The main processing pipeline applies a sequence of reactive operators to transform the stream. First, a backpressure buffer with a capacity of 5,000 items protects against overflow when the producer emits faster than the consumer can process.
The reduce operator aggregates all emitted values into a single sum, effectively counting total matches across all PDFs. This reduction happens asynchronously as values flow through the stream.
The pipeline concludes with a blocking subscription that waits for the final reduced value, executing success and error handlers when the stream completes.

Unlike the final aggregation performed by the reduce operator, the model is updated immediately within the emitter loop whenever a match is found. This update strategy provides real-time UI feedback through incremental model updates while the reduce operator handles final result computation.

.mermaid 
    flowchart TD
        Start[RxJavaSearcher] --> Create[Create Flowable<br/>with Custom Emitter]
        
        Create --> Loop[Iterate PDF Files<br/>in Emitter]
        
        Loop --> Process1[Process PDF 1<br/>containsWord]
        Loop --> Process2[Process PDF 2<br/>containsWord]
        Loop --> ProcessN[Process PDF N<br/>containsWord]
        
        Process1 --> Check1{Match Found?}
        Process2 --> Check2{Match Found?}
        ProcessN --> CheckN{Match Found?}
        
        Check1 -->|Yes| Update1[Update Model]
        Check2 -->|Yes| Update2[Update Model]
        CheckN -->|Yes| UpdateN[Update Model]
        
        Check1 -->|Always| Emit1[Emit 1 or 0]
        Check2 -->|Always| Emit2[Emit 1 or 0]
        CheckN -->|Always| EmitN[Emit 1 or 0]
        
        Update1 --> Emit1
        Update2 --> Emit2
        UpdateN --> EmitN
        
        Emit1 --> Hot[publish + connect<br/>Hot Observable]
        Emit2 --> Hot
        EmitN --> Hot
        
        Hot --> Buffer[onBackpressureBuffer<br/>capacity: 5000]
        Buffer --> Reduce[reduce<br/>sum all values]
        Reduce --> Schedule[observeOn<br/>Schedulers.computation]
        Schedule --> Subscribe[blockingSubscribe<br/>wait for result]
        Subscribe --> Final[Print Total Count<br/>+ Time]

<<<
## Actor approach 
The actor based implementation uses the Akka framework to model PDF processing as independent actors communicating through asynchronous message passing.

### Actors
The solution creates an Akka actor system named `PdfCounter` that serves as the runtime environment for all actors. Two actor types are instantiated: a single analyzer actor that processes all PDF files and maintains the match count, and a requester actor that retrieves the final result and terminates the system.

The analyzer actor is created first and receives its reference, which serves as its mailbox address for incoming messages. After all processing messages are sent, the requester actor is created and sent a query message to retrieve the accumulated count.

### Messages
The implementation defines two message types as immutable data classes. The processing message encapsulates a PDF file reference, the search word, and the model to update, bundling all necessary information for a single file analysis task. The query message is a simple marker class that signals the actor should respond with its current count.

Messages are sent using the tell method, which provides asynchronous delivery without blocking the sender. For processing messages, no sender reference is provided since no response is expected. For the query message, the requester actor's reference is provided so the analyzer knows where to send the final count.



### Actor behavior
The analyzer actor defines its behavior through a receive builder that pattern-matches incoming messages. When it receives a processing message, it executes the PDF text extraction, searches for the word, increments its internal count if a match is found, and updates the model.
When it receives a query message, it responds by sending its accumulated count back to the sender using the sender reference automatically captured from the incoming message context. 


.mermaid 
    flowchart TD
    Start([Start]) --> CreateSystem[Create ActorSystem<br/>PdfCounter]
    CreateSystem --> CreateAnalyzer[Create PdfAnalyzerActor]
    
    CreateAnalyzer --> SendLoop{More PDFs?}
    SendLoop -->|Yes| SendMsg[Send PdfWordMessage<br/>to Analyzer]
    SendMsg --> SendLoop
    SendLoop -->|No| CreateReq[Create RequesterActor]
    
    CreateReq --> SendQuery[Send GetCount<br/>to Analyzer with<br/>Requester ref]
    
    SendQuery --> AnalyzerProcess
    
    subgraph AnalyzerProcess[PdfAnalyzerActor Processing]
        ReceiveMsg{Receive<br/>Message}
        ReceiveMsg -->|PdfWordMessage| Load[Load PDF &<br/>Extract Text]
        Load --> Search{Contains<br/>Word?}
        Search -->|Yes| IncCount[Increment Count]
        Search -->|No| Skip[Skip]
        IncCount --> UpdateModel[Update Model]
        UpdateModel --> NextMsg[Next Message]
        Skip --> NextMsg
        NextMsg --> ReceiveMsg
        
        ReceiveMsg -->|GetCount| SendCount[Send Count<br/>to Requester]
    end
    
    SendCount --> RequesterReceive
    
    subgraph RequesterReceive[RequesterActor Processing]
        Receive[Receive Count]
        Receive --> LogResult[Log Result]
        LogResult --> Terminate[Terminate ActorSystem]
    end
    
    Terminate --> End([End])
    

<<<
# Performance









<!-- # Performance 

# Correctness

about the Thread based strategy, considering these are my notes:
- In the thread based approach I divided the chunks of pdfs based on the number of available physical CPUs in the moment so I created Threads based on that number:
`int Nthreads = Ncpu + 1;`

Then I implemented monitor with workers => one worker named Output waiting for the others to return the result
 -->


